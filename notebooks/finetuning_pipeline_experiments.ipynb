{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# Fine-Tuning Pipeline - Experiment Notebook\n",
        "\n",
        "Notebook for fine-tuning LLMs for NER extraction using LoRA or full fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "from loguru import logger\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from src.config import (\n",
        "    CHECKPOINTS_DIR,\n",
        "    PROCESSED_DATA_DIR,\n",
        "    RESULTS_DIR,\n",
        "    NERFineTuningConfig,\n",
        ")\n",
        "from src.finetuning_pipeline import FineTunedNERExtractor\n",
        "from src.data_processor import DataProcessor\n",
        "from src.utils import calculate_metrics, display_metrics\n",
        "\n",
        "logger.info(\"Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {},
      "source": [
        "## Training Configuration\n",
        "\n",
        "Configure fine-tuning parameters. You can train with LoRA (parameter-efficient) or full fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-4",
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"qwen3_4b_lora_r16\"\n",
        "\n",
        "config = NERFineTuningConfig(\n",
        "    # Model settings\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        "    load_in_8bit=False,\n",
        "    full_finetuning=False,\n",
        "    \n",
        "    # Prompt settings\n",
        "    add_schema=False,\n",
        "    enable_thinking=False,\n",
        "    \n",
        "    # Training settings\n",
        "    max_steps=1000,\n",
        "    num_epochs=3,\n",
        "    batch_size=4,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=100,\n",
        "    gradient_accumulation_steps=4,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    weight_decay=0.01,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    \n",
        "    # LoRA settings (only used if full_finetuning=False)\n",
        "    lora_r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    \n",
        "    # Logging and checkpointing\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "    eval_steps=100,\n",
        "    \n",
        "    # Output directory\n",
        "    output_dir=CHECKPOINTS_DIR / f\"{EXPERIMENT_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "    \n",
        "    # Data paths\n",
        "    train_data_path=PROCESSED_DATA_DIR / \"train_finetuning_chat.jsonl\",\n",
        "    val_data_path=PROCESSED_DATA_DIR / \"test_finetuning_chat.jsonl\",\n",
        "    \n",
        "    # Training options\n",
        "    resume_from_checkpoint=None,\n",
        "    report_to=\"none\",  # \"none\", \"wandb\", or \"tensorboard\"\n",
        ")\n",
        "\n",
        "print(\"Fine-Tuning Configuration:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Model: {config.model_name}\")\n",
        "print(f\"Max sequence length: {config.max_seq_length}\")\n",
        "print(f\"Quantization: {'4-bit' if config.load_in_4bit else '8-bit' if config.load_in_8bit else 'None'}\")\n",
        "print(f\"Training mode: {'Full fine-tuning' if config.full_finetuning else 'LoRA'}\")\n",
        "if not config.full_finetuning:\n",
        "    print(f\"  LoRA rank: {config.lora_r}\")\n",
        "    print(f\"  LoRA alpha: {config.lora_alpha}\")\n",
        "    print(f\"  LoRA dropout: {config.lora_dropout}\")\n",
        "print(f\"\\nTraining Settings:\")\n",
        "print(f\"  Max steps: {config.max_steps}\")\n",
        "print(f\"  Epochs: {config.num_epochs}\")\n",
        "print(f\"  Batch size: {config.batch_size}\")\n",
        "print(f\"  Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {config.learning_rate}\")\n",
        "print(f\"  Warmup steps: {config.warmup_steps}\")\n",
        "print(f\"  Weight decay: {config.weight_decay}\")\n",
        "print(f\"  Optimizer: {config.optim}\")\n",
        "print(f\"\\nOutput directory: {config.output_dir}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-5",
      "metadata": {},
      "source": [
        "## Validate Training Data\n",
        "\n",
        "Check that training and validation data exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check training data\n",
        "if not config.train_data_path.exists():\n",
        "    raise FileNotFoundError(f\"Training data not found: {config.train_data_path}\")\n",
        "else:\n",
        "    # Count training samples\n",
        "    with open(config.train_data_path, 'r') as f:\n",
        "        train_samples = sum(1 for _ in f)\n",
        "    logger.success(f\"Training data found: {train_samples} samples\")\n",
        "\n",
        "# Check validation data\n",
        "if not config.val_data_path.exists():\n",
        "    logger.warning(f\"Validation data not found: {config.val_data_path}\")\n",
        "    logger.warning(\"Training will proceed without validation\")\n",
        "    config.val_data_path = None\n",
        "    val_samples = 0\n",
        "else:\n",
        "    with open(config.val_data_path, 'r') as f:\n",
        "        val_samples = sum(1 for _ in f)\n",
        "    logger.success(f\"Validation data found: {val_samples} samples\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Training samples: {train_samples}\")\n",
        "print(f\"Validation samples: {val_samples}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Show sample training data\n",
        "print(\"\\nSample training data:\")\n",
        "with open(config.train_data_path, 'r') as f:\n",
        "    sample = json.loads(f.readline())\n",
        "    print(json.dumps(sample, indent=2, ensure_ascii=False)[:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {},
      "source": [
        "## Initialize Model\n",
        "\n",
        "Load the model and prepare for training. This will load the base model with the specified quantization settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-8",
      "metadata": {},
      "outputs": [],
      "source": [
        "logger.info(\"Initializing model...\")\n",
        "extractor = FineTunedNERExtractor(config)\n",
        "logger.success(\"Model initialized successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {},
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10",
      "metadata": {},
      "outputs": [],
      "source": [
        "logger.info(\"=\" * 80)\n",
        "logger.info(\"STARTING TRAINING\")\n",
        "logger.info(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Start training\n",
        "trainer_stats = extractor.train()\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "logger.success(\"=\" * 80)\n",
        "logger.success(\"TRAINING COMPLETE!\")\n",
        "logger.success(\"=\" * 80)\n",
        "logger.info(f\"Model saved to: {config.output_dir / 'final'}\")\n",
        "logger.info(f\"Total training time: {elapsed_time:.2f}s ({elapsed_time/60:.2f} minutes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {},
      "source": [
        "## Training Statistics\n",
        "\n",
        "Display training metrics and statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {},
      "outputs": [],
      "source": [
        "if trainer_stats:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING STATISTICS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    metrics = trainer_stats.metrics\n",
        "    \n",
        "    if 'train_runtime' in metrics:\n",
        "        runtime = metrics['train_runtime']\n",
        "        print(f\"Training runtime: {runtime:.2f}s ({runtime/60:.2f} minutes)\")\n",
        "    \n",
        "    if 'train_samples' in metrics:\n",
        "        print(f\"Training samples: {metrics['train_samples']}\")\n",
        "    \n",
        "    if 'train_steps_per_second' in metrics:\n",
        "        print(f\"Steps per second: {metrics['train_steps_per_second']:.2f}\")\n",
        "    \n",
        "    if 'train_samples_per_second' in metrics:\n",
        "        print(f\"Samples per second: {metrics['train_samples_per_second']:.2f}\")\n",
        "    \n",
        "    if 'train_loss' in metrics:\n",
        "        print(f\"\\nFinal training loss: {metrics['train_loss']:.4f}\")\n",
        "    \n",
        "    if 'eval_loss' in metrics:\n",
        "        print(f\"Final validation loss: {metrics['eval_loss']:.4f}\")\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Display all metrics\n",
        "    print(\"\\nAll metrics:\")\n",
        "    for key, value in sorted(metrics.items()):\n",
        "        print(f\"  {key}: {value}\")\n",
        "else:\n",
        "    print(\"No training statistics available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {},
      "source": [
        "## Load Test Dataset for Evaluation\n",
        "\n",
        "Load test data to evaluate the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataset_path = PROCESSED_DATA_DIR / \"test.json\"\n",
        "\n",
        "if not test_dataset_path.exists():\n",
        "    logger.warning(f\"Test dataset not found: {test_dataset_path}\")\n",
        "    logger.warning(\"Skipping evaluation\")\n",
        "    test_dataset = None\n",
        "else:\n",
        "    logger.info(f\"Loading test dataset from {test_dataset_path}\")\n",
        "    test_dataset = DataProcessor.load_dataset(test_dataset_path)\n",
        "    logger.success(f\"Loaded {len(test_dataset)} test samples\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TEST DATASET EXAMPLE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Text:\\n{test_dataset[0]['text'][:300]}...\\n\")\n",
        "    print(f\"Entities:\\n{json.dumps(test_dataset[0]['entities'], indent=2, ensure_ascii=False)}\")\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {},
      "source": [
        "## Test Single Sample\n",
        "\n",
        "Test the fine-tuned model on a single sample before full evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_dataset:\n",
        "    test_text = test_dataset[0][\"text\"]\n",
        "    test_label = test_dataset[0][\"entities\"]\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"SINGLE SAMPLE TEST - FINE-TUNED MODEL\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nInput text:\\n{test_text[:300]}...\\n\")\n",
        "    print(f\"Ground truth:\\n{json.dumps(test_label, indent=2, ensure_ascii=False)}\\n\")\n",
        "    \n",
        "    logger.info(\"Running extraction on test sample...\")\n",
        "    prediction = extractor.extract_entities(test_text)\n",
        "    \n",
        "    print(f\"Prediction:\\n{json.dumps(prediction, indent=2, ensure_ascii=False)}\")\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {},
      "source": [
        "## Run Full Evaluation\n",
        "\n",
        "Evaluate the fine-tuned model on the entire test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-18",
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_dataset:\n",
        "    # Prepare data\n",
        "    texts = [sample[\"text\"] for sample in test_dataset]\n",
        "    labels = [sample[\"entities\"] for sample in test_dataset]\n",
        "    \n",
        "    logger.info(f\"Starting evaluation on {len(test_dataset)} samples...\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Running evaluation on {len(test_dataset)} samples\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    eval_start_time = time.time()\n",
        "    predictions = []\n",
        "    \n",
        "    for i, (text, label) in enumerate(tqdm(zip(texts, labels), desc=\"Extracting entities\", total=len(texts))):\n",
        "        prediction = extractor.extract_entities(text)\n",
        "        predictions.append(prediction)\n",
        "        \n",
        "        # Show first few predictions for debugging\n",
        "        if i < 3:\n",
        "            logger.debug(f\"Sample {i+1}:\")\n",
        "            logger.debug(f\"  Ground truth: {label}\")\n",
        "            logger.debug(f\"  Prediction  : {prediction}\")\n",
        "    \n",
        "    eval_elapsed_time = time.time() - eval_start_time\n",
        "    throughput = len(test_dataset) / eval_elapsed_time if eval_elapsed_time > 0 else 0\n",
        "    avg_time = eval_elapsed_time / len(test_dataset)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"EVALUATION COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total samples: {len(test_dataset)}\")\n",
        "    print(f\"Total time: {eval_elapsed_time:.2f}s\")\n",
        "    print(f\"Throughput: {throughput:.2f} samples/s\")\n",
        "    print(f\"Avg time per sample: {avg_time:.2f}s\")\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    logger.warning(\"No test dataset available for evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-19",
      "metadata": {},
      "source": [
        "## Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-20",
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_dataset:\n",
        "    logger.info(\"Calculating metrics...\")\n",
        "    metrics = calculate_metrics(predictions, labels)\n",
        "    \n",
        "    display_metrics(metrics, title=f\"METRICS - {EXPERIMENT_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-21",
      "metadata": {},
      "source": [
        "## Analyze Sample Results\n",
        "\n",
        "Look at some examples to understand model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-22",
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_dataset:\n",
        "    # Analyze predictions\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SAMPLE PREDICTIONS ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Show first 3 samples\n",
        "    for i in range(min(3, len(test_dataset))):\n",
        "        print(f\"\\nSample {i+1}:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Text: {texts[i][:200]}...\")\n",
        "        print(\"\\nGround Truth:\")\n",
        "        print(json.dumps(labels[i], indent=2, ensure_ascii=False))\n",
        "        print(\"\\nPrediction:\")\n",
        "        print(json.dumps(predictions[i], indent=2, ensure_ascii=False))\n",
        "        print(\"-\" * 80)\n",
        "    \n",
        "    # Find errors\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ERROR ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    error_count = 0\n",
        "    for i, (pred, truth) in enumerate(zip(predictions, labels)):\n",
        "        has_error = False\n",
        "        for entity_type in [\"person\", \"organizations\", \"address\"]:\n",
        "            pred_set = set(pred.get(entity_type, []))\n",
        "            truth_set = set(truth.get(entity_type, []))\n",
        "            if pred_set != truth_set:\n",
        "                has_error = True\n",
        "                break\n",
        "        \n",
        "        if has_error:\n",
        "            error_count += 1\n",
        "    \n",
        "    accuracy = (len(test_dataset) - error_count) / len(test_dataset) * 100\n",
        "    print(f\"Samples with errors: {error_count} / {len(test_dataset)}\")\n",
        "    print(f\"Perfect match accuracy: {accuracy:.2f}%\")\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-23",
      "metadata": {},
      "source": [
        "## Save Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-24",
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_dataset:\n",
        "    # Save results\n",
        "    results_dir = RESULTS_DIR / \"finetuning_eval\" / EXPERIMENT_NAME\n",
        "    results_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    results = {\n",
        "        \"experiment_name\": EXPERIMENT_NAME,\n",
        "        \"config\": {\n",
        "            \"model_name\": config.model_name,\n",
        "            \"max_seq_length\": config.max_seq_length,\n",
        "            \"load_in_4bit\": config.load_in_4bit,\n",
        "            \"full_finetuning\": config.full_finetuning,\n",
        "            \"lora_r\": config.lora_r if not config.full_finetuning else None,\n",
        "            \"lora_alpha\": config.lora_alpha if not config.full_finetuning else None,\n",
        "            \"learning_rate\": config.learning_rate,\n",
        "            \"batch_size\": config.batch_size,\n",
        "            \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
        "        },\n",
        "        \"training_stats\": trainer_stats.metrics if trainer_stats else {},\n",
        "        \"evaluation_metrics\": metrics,\n",
        "        \"evaluation_performance\": {\n",
        "            \"total_samples\": len(test_dataset),\n",
        "            \"elapsed_time\": round(eval_elapsed_time, 2),\n",
        "            \"throughput\": round(throughput, 2),\n",
        "            \"avg_time_per_sample\": round(avg_time, 2),\n",
        "        },\n",
        "    }\n",
        "    \n",
        "    results_file = results_dir / \"evaluation_results.json\"\n",
        "    with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    logger.success(f\"Results saved to {results_file}\")\n",
        "    print(f\"\\n✅ Evaluation results saved to: {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-25",
      "metadata": {},
      "source": [
        "## Model Information\n",
        "\n",
        "Summary of the fine-tuned model location and how to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-26",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FINE-TUNED MODEL INFORMATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Model saved at: {config.output_dir / 'final'}\")\n",
        "print(f\"\\nTo use this model later:\")\n",
        "print(f\"\"\"\\n\n",
        "from src.finetuning import FineTunedNERExtractor\n",
        "from src.config import NERFineTuningConfig\n",
        "\n",
        "config = NERFineTuningConfig(\n",
        "    model_name=\"{config.model_name}\",\n",
        "    trained_model_path=\"{config.output_dir / 'final'}\",\n",
        "    load_in_4bit={config.load_in_4bit},\n",
        ")\n",
        "\n",
        "extractor = FineTunedNERExtractor(config)\n",
        "entities = extractor.extract_entities(\"Your text here\")\n",
        "\"\"\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-27",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Configuration Templates\n",
        "\n",
        "Copy these configurations into the \"Training Configuration\" cell to try different setups:\n",
        "\n",
        "### 1. LoRA Training (Default - Memory Efficient)\n",
        "```python\n",
        "EXPERIMENT_NAME = \"lora_r16_efficient\"\n",
        "config = NERFineTuningConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    load_in_4bit=True,\n",
        "    full_finetuning=False,\n",
        "    lora_r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    batch_size=4,\n",
        "    learning_rate=1e-4,\n",
        "    max_steps=1000,\n",
        ")\n",
        "```\n",
        "\n",
        "### 2. Higher Rank LoRA (Better Performance)\n",
        "```python\n",
        "EXPERIMENT_NAME = \"lora_r64_high_rank\"\n",
        "config = NERFineTuningConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    load_in_4bit=True,\n",
        "    full_finetuning=False,\n",
        "    lora_r=64,\n",
        "    lora_alpha=128,\n",
        "    lora_dropout=0.05,\n",
        "    batch_size=4,\n",
        "    learning_rate=2e-4,\n",
        "    max_steps=1000,\n",
        ")\n",
        "```\n",
        "\n",
        "### 3. Full Fine-Tuning (Best Performance, Requires More Memory)\n",
        "```python\n",
        "EXPERIMENT_NAME = \"full_finetuning\"\n",
        "config = NERFineTuningConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    load_in_4bit=False,\n",
        "    load_in_8bit=True,\n",
        "    full_finetuning=True,\n",
        "    batch_size=2,\n",
        "    learning_rate=5e-5,\n",
        "    max_steps=1000,\n",
        ")\n",
        "```\n",
        "\n",
        "### 4. Quick Training (Fast Testing)\n",
        "```python\n",
        "EXPERIMENT_NAME = \"quick_test\"\n",
        "config = NERFineTuningConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    load_in_4bit=True,\n",
        "    full_finetuning=False,\n",
        "    lora_r=8,\n",
        "    batch_size=8,\n",
        "    learning_rate=2e-4,\n",
        "    max_steps=100,\n",
        "    save_steps=50,\n",
        ")\n",
        "```\n",
        "\n",
        "### 5. With Schema in Prompt\n",
        "```python\n",
        "EXPERIMENT_NAME = \"lora_with_schema\"\n",
        "config = NERFineTuningConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    load_in_4bit=True,\n",
        "    full_finetuning=False,\n",
        "    add_schema=True,\n",
        "    lora_r=16,\n",
        "    batch_size=4,\n",
        "    learning_rate=1e-4,\n",
        "    max_steps=1000,\n",
        ")\n",
        "```\n",
        "\n",
        "### 6. With Thinking Mode\n",
        "```python\n",
        "EXPERIMENT_NAME = \"lora_thinking_mode\"\n",
        "config = NERFineTuningConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    load_in_4bit=True,\n",
        "    full_finetuning=False,\n",
        "    enable_thinking=True,\n",
        "    lora_r=16,\n",
        "    batch_size=4,\n",
        "    learning_rate=1e-4,\n",
        "    max_steps=1000,\n",
        ")\n",
        "```\n",
        "\n",
        "### Tips:\n",
        "\n",
        "**Memory Usage:**\n",
        "- 4-bit quantization: ~4-6GB VRAM\n",
        "- 8-bit quantization: ~8-10GB VRAM\n",
        "- Full precision: ~16GB+ VRAM\n",
        "\n",
        "**LoRA Rank:**\n",
        "- r=8: Fast, less parameters, good for simple tasks\n",
        "- r=16: Balanced (recommended)\n",
        "- r=64: More expressive, better for complex tasks\n",
        "\n",
        "**Learning Rate:**\n",
        "- Full fine-tuning: 5e-5 to 1e-4\n",
        "- LoRA: 1e-4 to 3e-4\n",
        "\n",
        "**Batch Size:**\n",
        "- Use gradient accumulation for larger effective batch sizes\n",
        "- Effective batch size = batch_size × gradient_accumulation_steps\n",
        "- Target: 16-32 effective batch size"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
