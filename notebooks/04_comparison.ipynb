{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of All Three Methods\n",
    "\n",
    "This notebook compares results from all three NER extraction methods:\n",
    "1. Prompt Engineering\n",
    "2. RAG (Retrieval-Augmented Generation)\n",
    "3. Fine-tuning\n",
    "\n",
    "## Comparison Metrics\n",
    "- Exact Match Accuracy\n",
    "- Precision, Recall, F1 per entity type\n",
    "- Inference speed\n",
    "- Resource requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import RESULTS_DIR\n",
    "from src.evaluation import compare_methods\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Results from All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "results_dict = {}\n",
    "\n",
    "methods = {\n",
    "    \"Prompt Engineering\": RESULTS_DIR / \"prompt_engineering\" / \"Prompt Engineering_results.json\",\n",
    "    \"RAG\": RESULTS_DIR / \"rag\" / \"RAG_results.json\",\n",
    "    \"Fine-tuning\": RESULTS_DIR / \"finetuning\" / \"Fine-tuning_results.json\",\n",
    "}\n",
    "\n",
    "for method_name, result_path in methods.items():\n",
    "    if result_path.exists():\n",
    "        with open(result_path, \"r\") as f:\n",
    "            results_dict[method_name] = json.load(f)\n",
    "        print(f\"âœ“ Loaded {method_name} results\")\n",
    "    else:\n",
    "        print(f\"âœ— {method_name} results not found at {result_path}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(results_dict)} method results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = compare_methods(results_dict)\n",
    "\n",
    "# Add performance metrics\n",
    "perf_data = []\n",
    "for method_name, results in results_dict.items():\n",
    "    perf_data.append({\n",
    "        \"Method\": method_name,\n",
    "        \"Inference Time (s)\": results.get(\"inference_time\", 0),\n",
    "        \"Samples/Second\": results.get(\"samples_per_second\", 0),\n",
    "    })\n",
    "\n",
    "perf_df = pd.DataFrame(perf_data)\n",
    "final_df = pd.merge(comparison_df, perf_df, on=\"Method\")\n",
    "\n",
    "# Display key columns\n",
    "display_cols = [\n",
    "    \"Method\",\n",
    "    \"Exact Match Accuracy\",\n",
    "    \"macro_f1\",\n",
    "    \"macro_precision\",\n",
    "    \"macro_recall\",\n",
    "    \"Inference Time (s)\",\n",
    "    \"Samples/Second\"\n",
    "]\n",
    "\n",
    "display_df = final_df[display_cols].copy()\n",
    "display_df.columns = [\"Method\", \"Exact Match\", \"F1\", \"Precision\", \"Recall\", \"Time (s)\", \"Samples/s\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPARISON OF ALL METHODS\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "final_df.to_csv(RESULTS_DIR / \"full_comparison.csv\", index=False)\n",
    "print(f\"\\nComparison saved to {RESULTS_DIR / 'full_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Exact Match Accuracy\n",
    "axes[0].bar(display_df[\"Method\"], display_df[\"Exact Match\"], color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "axes[0].set_ylabel(\"Exact Match Accuracy\")\n",
    "axes[0].set_title(\"Exact Match Accuracy by Method\")\n",
    "axes[0].set_ylim(0, 1)\n",
    "for i, v in enumerate(display_df[\"Exact Match\"]):\n",
    "    axes[0].text(i, v + 0.02, f\"{v:.3f}\", ha='center', va='bottom')\n",
    "\n",
    "# Macro F1 Score\n",
    "axes[1].bar(display_df[\"Method\"], display_df[\"F1\"], color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "axes[1].set_ylabel(\"Macro F1 Score\")\n",
    "axes[1].set_title(\"Macro F1 Score by Method\")\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i, v in enumerate(display_df[\"F1\"]):\n",
    "    axes[1].text(i, v + 0.02, f\"{v:.3f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"accuracy_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Entity Type Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-entity metrics\n",
    "entity_types = [\"person\", \"organizations\", \"address\"]\n",
    "metrics_data = []\n",
    "\n",
    "for method_name, results in results_dict.items():\n",
    "    partial_metrics = results[\"partial_match_metrics\"]\n",
    "    for entity_type in entity_types:\n",
    "        if entity_type in partial_metrics:\n",
    "            metrics = partial_metrics[entity_type]\n",
    "            metrics_data.append({\n",
    "                \"Method\": method_name,\n",
    "                \"Entity Type\": entity_type,\n",
    "                \"Precision\": metrics[\"precision\"],\n",
    "                \"Recall\": metrics[\"recall\"],\n",
    "                \"F1\": metrics[\"f1\"]\n",
    "            })\n",
    "\n",
    "entity_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Plot F1 scores by entity type\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "entity_pivot = entity_df.pivot(index=\"Entity Type\", columns=\"Method\", values=\"F1\")\n",
    "entity_pivot.plot(kind=\"bar\", ax=ax, width=0.8)\n",
    "\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(\"F1 Score by Entity Type and Method\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(title=\"Method\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"entity_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-Entity F1 Scores:\")\n",
    "print(entity_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.barh(display_df[\"Method\"], display_df[\"Samples/s\"], color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "ax.set_xlabel(\"Samples per Second\")\n",
    "ax.set_title(\"Inference Speed Comparison\")\n",
    "\n",
    "for i, v in enumerate(display_df[\"Samples/s\"]):\n",
    "    ax.text(v + 0.01, i, f\"{v:.2f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"speed_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Precision-Recall Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors = {'Prompt Engineering': '#3498db', 'RAG': '#e74c3c', 'Fine-tuning': '#2ecc71'}\n",
    "markers = {'Prompt Engineering': 'o', 'RAG': 's', 'Fine-tuning': '^'}\n",
    "\n",
    "for method_name in results_dict.keys():\n",
    "    method_data = entity_df[entity_df[\"Method\"] == method_name]\n",
    "    \n",
    "    ax.scatter(\n",
    "        method_data[\"Recall\"],\n",
    "        method_data[\"Precision\"],\n",
    "        s=200,\n",
    "        label=method_name,\n",
    "        color=colors.get(method_name, 'gray'),\n",
    "        marker=markers.get(method_name, 'o'),\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Annotate entity types\n",
    "    for _, row in method_data.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"Entity Type\"][:3].upper(),\n",
    "            (row[\"Recall\"], row[\"Precision\"]),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=8\n",
    "        )\n",
    "\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.set_title(\"Precision-Recall Comparison\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 1.05)\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "# Add diagonal line for F1=0.5\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"precision_recall.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "\n",
    "# Find best method for each metric\n",
    "best_accuracy = display_df.loc[display_df[\"Exact Match\"].idxmax()]\n",
    "best_f1 = display_df.loc[display_df[\"F1\"].idxmax()]\n",
    "fastest = display_df.loc[display_df[\"Samples/s\"].idxmax()]\n",
    "\n",
    "print(\"ðŸ† Best Performance:\")\n",
    "print(f\"  â€¢ Highest Exact Match Accuracy: {best_accuracy['Method']} ({best_accuracy['Exact Match']:.2%})\")\n",
    "print(f\"  â€¢ Highest Macro F1: {best_f1['Method']} ({best_f1['F1']:.2%})\")\n",
    "print(f\"  â€¢ Fastest Inference: {fastest['Method']} ({fastest['Samples/s']:.2f} samples/s)\")\n",
    "\n",
    "print(\"\\nðŸ“Š Method Characteristics:\\n\")\n",
    "\n",
    "print(\"1. Prompt Engineering:\")\n",
    "print(\"   âœ“ Quick to implement (no training)\")\n",
    "print(\"   âœ“ Good baseline performance\")\n",
    "print(\"   âœ— May be inconsistent with complex text\")\n",
    "print(\"   â†’ Best for: Quick prototyping, simple use cases\")\n",
    "\n",
    "print(\"\\n2. RAG (Retrieval-Augmented Generation):\")\n",
    "print(\"   âœ“ Better accuracy with context\")\n",
    "print(\"   âœ“ Reduced hallucination\")\n",
    "print(\"   âœ— Slower due to retrieval overhead\")\n",
    "print(\"   â†’ Best for: Cases with good example corpus, domain adaptation\")\n",
    "\n",
    "print(\"\\n3. Fine-tuning:\")\n",
    "print(\"   âœ“ Highest accuracy for domain-specific patterns\")\n",
    "print(\"   âœ“ Consistent predictions\")\n",
    "print(\"   âœ— Requires training time and GPU\")\n",
    "print(\"   â†’ Best for: Production deployment, domain-specific requirements\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Final Recommendation:\")\n",
    "if len(results_dict) >= 3:\n",
    "    if best_f1['Method'] == 'Fine-tuning':\n",
    "        print(\"   Fine-tuning shows the best overall performance and should be used\")\n",
    "        print(\"   for production deployment where accuracy is critical.\")\n",
    "    else:\n",
    "        print(f\"   {best_f1['Method']} shows the best F1 score and provides a good\")\n",
    "        print(\"   balance between performance and implementation complexity.\")\n",
    "else:\n",
    "    print(\"   Run all three methods to get complete comparison.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "report = {\n",
    "    \"comparison_table\": final_df.to_dict(orient=\"records\"),\n",
    "    \"per_entity_metrics\": entity_df.to_dict(orient=\"records\"),\n",
    "    \"best_methods\": {\n",
    "        \"exact_match\": best_accuracy[\"Method\"],\n",
    "        \"f1_score\": best_f1[\"Method\"],\n",
    "        \"fastest\": fastest[\"Method\"]\n",
    "    },\n",
    "    \"detailed_results\": results_dict\n",
    "}\n",
    "\n",
    "report_path = RESULTS_DIR / \"final_report.json\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nFinal report saved to {report_path}\")\n",
    "print(\"\\nAll comparison visualizations saved:\")\n",
    "print(f\"  â€¢ {RESULTS_DIR / 'accuracy_comparison.png'}\")\n",
    "print(f\"  â€¢ {RESULTS_DIR / 'entity_comparison.png'}\")\n",
    "print(f\"  â€¢ {RESULTS_DIR / 'speed_comparison.png'}\")\n",
    "print(f\"  â€¢ {RESULTS_DIR / 'precision_recall.png'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
