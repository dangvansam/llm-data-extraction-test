{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation: Entity Extraction with Langextract\n",
    "\n",
    "This notebook prepares the training and test datasets by extracting named entities from raw Vietnamese news articles using Google's Langextract with Gemini API.\n",
    "\n",
    "## Features\n",
    "- ‚úÖ Parallel processing (4-8x faster extraction)\n",
    "- ‚úÖ Automatic resume on crash\n",
    "- ‚úÖ Incremental saving (no data loss)\n",
    "- ‚úÖ Multi-encoding support (UTF-8, Latin-1, CP1252, UTF-16)\n",
    "- ‚úÖ Progress tracking\n",
    "\n",
    "## Output\n",
    "- Individual JSON files: `data/processed/{split}/{category}/json/{article}.json`\n",
    "- Combined datasets: `langextract_train.json`, `langextract_test.json`\n",
    "- Finetuning format: `langextract_train_finetuning.jsonl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "from loguru import logger\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.config import PROCESSED_DATA_DIR, RAW_DATA_DIR, NERLangExtractConfig\n",
    "from src.langextract_pipeline import LangExtractNERExtractor\n",
    "from src.services.data_processor import DataProcessorService"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if API key is set\n",
    "if not os.getenv(\"GEMINI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è GEMINI_API_KEY not found!\")\n",
    "    print(\"Please set it: export GEMINI_API_KEY='your-key-here'\")\n",
    "    print(\"Or create a .env file with: GEMINI_API_KEY=your-key-here\")\n",
    "else:\n",
    "    print(\"‚úÖ GEMINI_API_KEY is set\")\n",
    "\n",
    "# Configuration\n",
    "config = NERLangExtractConfig()\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Model: {config.model_id}\")\n",
    "print(f\"  Extraction passes: {config.extraction_passes}\")\n",
    "print(f\"  Max workers: {config.max_workers}\")\n",
    "print(f\"  Max char buffer: {config.max_char_buffer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Available Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_categories(split: str) -> List[str]:\n",
    "    \"\"\"Get list of available categories in a split.\"\"\"\n",
    "    split_path = RAW_DATA_DIR / split\n",
    "    if not split_path.exists():\n",
    "        return []\n",
    "    categories = [d.name for d in split_path.iterdir() if d.is_dir()]\n",
    "    return sorted(categories)\n",
    "\n",
    "def count_articles(split: str, category: str) -> int:\n",
    "    \"\"\"Count articles in a category.\"\"\"\n",
    "    category_path = RAW_DATA_DIR / split / category\n",
    "    if not category_path.exists():\n",
    "        return 0\n",
    "    return len(list(category_path.glob(\"*.txt\")))\n",
    "\n",
    "# Show available data\n",
    "print(\"Available Categories:\\n\")\n",
    "for split in [\"train\", \"test\"]:\n",
    "    categories = get_available_categories(split)\n",
    "    print(f\"{split.upper()}:\")\n",
    "    for cat in categories:\n",
    "        count = count_articles(split, cat)\n",
    "        print(f\"  - {cat}: {count} articles\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check Processing Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_processed_samples(split: str, category: str, output_dir: Path = PROCESSED_DATA_DIR) -> int:\n",
    "    \"\"\"Count how many samples have already been processed.\"\"\"\n",
    "    category_output_dir = output_dir / split / category / \"json\"\n",
    "    if not category_output_dir.exists():\n",
    "        return 0\n",
    "    return len(list(category_output_dir.glob(\"*.json\")))\n",
    "\n",
    "def show_progress():\n",
    "    \"\"\"Show processing progress for all categories.\"\"\"\n",
    "    print(\"Processing Progress:\\n\")\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        categories = get_available_categories(split)\n",
    "        print(f\"{split.upper()}:\")\n",
    "        for cat in categories:\n",
    "            total = count_articles(split, cat)\n",
    "            processed = count_processed_samples(split, cat)\n",
    "            percentage = (processed / total * 100) if total > 0 else 0\n",
    "            status = \"‚úÖ\" if processed == total else \"‚è≥\" if processed > 0 else \"‚ùå\"\n",
    "            print(f\"  {status} {cat}: {processed}/{total} ({percentage:.1f}%)\")\n",
    "        print()\n",
    "\n",
    "show_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Extraction on Sample Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single article first\n",
    "def test_single_article(split=\"train\", category=\"Doi song\"):\n",
    "    \"\"\"Test extraction on a single article.\"\"\"\n",
    "    category_path = RAW_DATA_DIR / split / category\n",
    "    \n",
    "    if not category_path.exists():\n",
    "        print(f\"‚ùå Category path not found: {category_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load first article\n",
    "    articles = DataProcessorService.load_articles_from_folder(category_path)\n",
    "    \n",
    "    if not articles:\n",
    "        print(\"‚ùå No articles found\")\n",
    "        return\n",
    "    \n",
    "    article = articles[0]\n",
    "    print(f\"Testing on: {article['file_name']}\")\n",
    "    print(f\"Text length: {len(article['text'])} characters\")\n",
    "    print(f\"\\nText preview:\\n{article['text'][:300]}...\\n\")\n",
    "    \n",
    "    # Extract entities\n",
    "    extractor = LangExtractNERExtractor(config=config)\n",
    "    print(\"Extracting entities...\")\n",
    "    \n",
    "    entities = extractor.extract_entities(\n",
    "        text=article[\"text\"],\n",
    "        extraction_passes=2,\n",
    "        max_workers=4\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Extracted entities:\")\n",
    "    print(f\"  Person ({len(entities.get('person', []))}): {entities.get('person', [])}\")\n",
    "    print(f\"  Organizations ({len(entities.get('organizations', []))}): {entities.get('organizations', [])}\")\n",
    "    print(f\"  Address ({len(entities.get('address', []))}): {entities.get('address', [])}\")\n",
    "    \n",
    "    total = sum(len(v) for v in entities.values())\n",
    "    print(f\"\\nTotal entities: {total}\")\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Run test\n",
    "test_entities = test_single_article()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process Single Article with Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_article(\n",
    "    article: dict,\n",
    "    category_name: str,\n",
    "    split: str,\n",
    "    output_dir: Path,\n",
    "    extractor: LangExtractNERExtractor,\n",
    "    extraction_passes: int = 2,\n",
    "    max_workers: int = 4\n",
    ") -> dict:\n",
    "    \"\"\"Process a single article and save to individual JSON file.\"\"\"\n",
    "    # Create output directory\n",
    "    category_output_dir = output_dir / split / category_name / \"json\"\n",
    "    category_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate output filename\n",
    "    file_stem = Path(article[\"file_name\"]).stem\n",
    "    output_file = category_output_dir / f\"{file_stem}.json\"\n",
    "    \n",
    "    # Check if already processed\n",
    "    if output_file.exists():\n",
    "        try:\n",
    "            with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                sample = json.load(f)\n",
    "            return sample\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load cached result for {article['file_name']}: {e}\")\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = extractor.extract_entities(\n",
    "        text=article[\"text\"],\n",
    "        extraction_passes=extraction_passes,\n",
    "        max_workers=max_workers\n",
    "    )\n",
    "    \n",
    "    # Create sample\n",
    "    sample = {\n",
    "        \"file_name\": article[\"file_name\"],\n",
    "        \"category\": category_name,\n",
    "        \"text\": article[\"text\"],\n",
    "        \"entities\": entities\n",
    "    }\n",
    "    \n",
    "    # Save individual result\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sample, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process Category with Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category(\n",
    "    category_name: str,\n",
    "    split: str,\n",
    "    extraction_passes: int = 2,\n",
    "    max_workers: int = 4,\n",
    "    output_dir: Path = PROCESSED_DATA_DIR\n",
    ") -> List[dict]:\n",
    "    \"\"\"Process all articles in a category with parallel processing.\"\"\"\n",
    "    category_path = RAW_DATA_DIR / split / category_name\n",
    "    \n",
    "    if not category_path.exists():\n",
    "        print(f\"‚ùå Category path not found: {category_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing: {category_name} ({split})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Load articles\n",
    "    articles = DataProcessorService.load_articles_from_folder(category_path)\n",
    "    \n",
    "    if not articles:\n",
    "        print(\"‚ùå No valid articles found\")\n",
    "        return []\n",
    "    \n",
    "    # Check for already processed samples\n",
    "    processed_count = count_processed_samples(split, category_name, output_dir)\n",
    "    if processed_count > 0:\n",
    "        print(f\"‚úÖ Found {processed_count} already processed samples - will skip those\")\n",
    "    \n",
    "    # Initialize extractor\n",
    "    extractor = LangExtractNERExtractor(config=config)\n",
    "    \n",
    "    # Process articles in parallel\n",
    "    samples = []\n",
    "    failed_articles = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_article = {\n",
    "            executor.submit(\n",
    "                process_single_article,\n",
    "                article,\n",
    "                category_name,\n",
    "                split,\n",
    "                output_dir,\n",
    "                extractor,\n",
    "                extraction_passes,\n",
    "                1  # Use 1 worker per article since we're parallelizing at article level\n",
    "            ): article\n",
    "            for article in articles\n",
    "        }\n",
    "        \n",
    "        # Process completed tasks with progress bar\n",
    "        for future in tqdm(as_completed(future_to_article), total=len(articles), desc=category_name):\n",
    "            article = future_to_article[future]\n",
    "            try:\n",
    "                sample = future.result()\n",
    "                samples.append(sample)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Failed to extract from {article['file_name']}: {e}\")\n",
    "                failed_articles.append(article[\"file_name\"])\n",
    "    \n",
    "    if failed_articles:\n",
    "        print(f\"\\n‚ö†Ô∏è Failed articles ({len(failed_articles)}): {', '.join(failed_articles[:5])}{'...' if len(failed_articles) > 5 else ''}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Processed {len(samples)}/{len(articles)} articles from {category_name}\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Process Selected Categories\n",
    "\n",
    "Choose which categories to process. Start with one category to test, then process all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure processing\n",
    "CATEGORIES_TO_PROCESS = [\"Doi song\"]  # Start with one category\n",
    "# CATEGORIES_TO_PROCESS = get_available_categories(\"train\")  # Or process all\n",
    "\n",
    "EXTRACTION_PASSES = 2  # More passes = better quality but slower\n",
    "MAX_WORKERS = 4  # More workers = faster but may hit rate limits\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Categories: {CATEGORIES_TO_PROCESS}\")\n",
    "print(f\"  Extraction passes: {EXTRACTION_PASSES}\")\n",
    "print(f\"  Max workers: {MAX_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Process Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = []\n",
    "\n",
    "for category in CATEGORIES_TO_PROCESS:\n",
    "    samples = process_category(\n",
    "        category_name=category,\n",
    "        split=\"train\",\n",
    "        extraction_passes=EXTRACTION_PASSES,\n",
    "        max_workers=MAX_WORKERS\n",
    "    )\n",
    "    train_samples.extend(samples)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total training samples: {len(train_samples)}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Process Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = []\n",
    "\n",
    "for category in CATEGORIES_TO_PROCESS:\n",
    "    samples = process_category(\n",
    "        category_name=category,\n",
    "        split=\"test\",\n",
    "        extraction_passes=EXTRACTION_PASSES,\n",
    "        max_workers=MAX_WORKERS\n",
    "    )\n",
    "    test_samples.extend(samples)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total test samples: {len(test_samples)}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Combined Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_samples:\n",
    "    # Deduplicate entities\n",
    "    train_samples = DataProcessorService.deduplicate_entities(train_samples)\n",
    "    \n",
    "    # Validate\n",
    "    valid_train, invalid_train = DataProcessorService.validate_samples(train_samples)\n",
    "    print(f\"Train: {len(valid_train)} valid, {len(invalid_train)} invalid\")\n",
    "    \n",
    "    # Save in different formats\n",
    "    train_json_path = PROCESSED_DATA_DIR / \"langextract_train.json\"\n",
    "    DataProcessorService.save_dataset(valid_train, train_json_path)\n",
    "    \n",
    "    train_jsonl_path = PROCESSED_DATA_DIR / \"langextract_train.jsonl\"\n",
    "    DataProcessorService.save_jsonl(valid_train, train_jsonl_path)\n",
    "    \n",
    "    train_finetuning_path = PROCESSED_DATA_DIR / \"langextract_train_finetuning.jsonl\"\n",
    "    DataProcessorService.export_for_finetuning(valid_train, train_finetuning_path, format=\"chat\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved training data to:\")\n",
    "    print(f\"  - {train_json_path}\")\n",
    "    print(f\"  - {train_jsonl_path}\")\n",
    "    print(f\"  - {train_finetuning_path}\")\n",
    "\n",
    "if test_samples:\n",
    "    # Deduplicate entities\n",
    "    test_samples = DataProcessorService.deduplicate_entities(test_samples)\n",
    "    \n",
    "    # Validate\n",
    "    valid_test, invalid_test = DataProcessorService.validate_samples(test_samples)\n",
    "    print(f\"\\nTest: {len(valid_test)} valid, {len(invalid_test)} invalid\")\n",
    "    \n",
    "    # Save in different formats\n",
    "    test_json_path = PROCESSED_DATA_DIR / \"langextract_test.json\"\n",
    "    DataProcessorService.save_dataset(valid_test, test_json_path)\n",
    "    \n",
    "    test_jsonl_path = PROCESSED_DATA_DIR / \"langextract_test.jsonl\"\n",
    "    DataProcessorService.save_jsonl(valid_test, test_jsonl_path)\n",
    "    \n",
    "    test_finetuning_path = PROCESSED_DATA_DIR / \"langextract_test_finetuning.jsonl\"\n",
    "    DataProcessorService.export_for_finetuning(valid_test, test_finetuning_path, format=\"chat\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved test data to:\")\n",
    "    print(f\"  - {test_json_path}\")\n",
    "    print(f\"  - {test_jsonl_path}\")\n",
    "    print(f\"  - {test_finetuning_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compute and Display Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def display_statistics(samples: List[dict], split_name: str):\n",
    "    \"\"\"Display statistics about the dataset.\"\"\"\n",
    "    stats = DataProcessorService.compute_statistics(samples)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{split_name.upper()} STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total samples: {stats['total_samples']}\")\n",
    "    print(f\"Samples with entities: {stats['samples_with_entities']}\")\n",
    "    print(f\"Samples without entities: {stats['samples_without_entities']}\")\n",
    "    print(f\"\\nEntity Counts:\")\n",
    "    print(f\"  Person: {stats['entity_counts']['person']}\")\n",
    "    print(f\"  Organizations: {stats['entity_counts']['organizations']}\")\n",
    "    print(f\"  Address: {stats['entity_counts']['address']}\")\n",
    "    print(f\"  Total: {stats['total_entities']}\")\n",
    "    print(f\"\\nAverages:\")\n",
    "    print(f\"  Entities per sample: {stats['avg_entities_per_sample']:.2f}\")\n",
    "    print(f\"  Text length: {stats['avg_text_length']:.0f} chars\")\n",
    "    print(f\"\\nText Length Range:\")\n",
    "    print(f\"  Min: {stats['min_text_length']} chars\")\n",
    "    print(f\"  Max: {stats['max_text_length']} chars\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Display statistics\n",
    "train_stats = None\n",
    "test_stats = None\n",
    "\n",
    "if train_samples:\n",
    "    train_stats = display_statistics(valid_train, \"train\")\n",
    "\n",
    "if test_samples:\n",
    "    test_stats = display_statistics(valid_test, \"test\")\n",
    "\n",
    "# Save combined statistics\n",
    "if train_stats or test_stats:\n",
    "    combined_stats = {}\n",
    "    if train_stats:\n",
    "        combined_stats[\"train\"] = train_stats\n",
    "    if test_stats:\n",
    "        combined_stats[\"test\"] = test_stats\n",
    "    \n",
    "    stats_path = PROCESSED_DATA_DIR / \"langextract_statistics.json\"\n",
    "    with open(stats_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined_stats, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Statistics saved to: {stats_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Entity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_entity_distribution(train_stats, test_stats):\n",
    "    \"\"\"Plot entity distribution for train and test sets.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Train distribution\n",
    "    if train_stats:\n",
    "        train_counts = train_stats['entity_counts']\n",
    "        axes[0].bar(train_counts.keys(), train_counts.values(), color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "        axes[0].set_title('Training Set - Entity Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_ylabel('Count', fontsize=12)\n",
    "        axes[0].set_xlabel('Entity Type', fontsize=12)\n",
    "        \n",
    "        for i, (k, v) in enumerate(train_counts.items()):\n",
    "            axes[0].text(i, v, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Test distribution\n",
    "    if test_stats:\n",
    "        test_counts = test_stats['entity_counts']\n",
    "        axes[1].bar(test_counts.keys(), test_counts.values(), color=['#3498db', '#e74c3c', '#2ecc71'])\n",
    "        axes[1].set_title('Test Set - Entity Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_ylabel('Count', fontsize=12)\n",
    "        axes[1].set_xlabel('Entity Type', fontsize=12)\n",
    "        \n",
    "        for i, (k, v) in enumerate(test_counts.items()):\n",
    "            axes[1].text(i, v, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot\n",
    "if train_stats or test_stats:\n",
    "    plot_entity_distribution(train_stats, test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sample Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few sample extractions\n",
    "def show_samples(samples: List[dict], n: int = 3):\n",
    "    \"\"\"Display sample extractions.\"\"\"\n",
    "    import random\n",
    "    \n",
    "    sample_items = random.sample(samples, min(n, len(samples)))\n",
    "    \n",
    "    for i, sample in enumerate(sample_items, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Sample {i}: {sample['file_name']}\")\n",
    "        print(f\"Category: {sample['category']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"\\nText preview:\\n{sample['text'][:300]}...\")\n",
    "        print(f\"\\nExtracted Entities:\")\n",
    "        print(f\"  Person ({len(sample['entities']['person'])}): {sample['entities']['person']}\")\n",
    "        print(f\"  Organizations ({len(sample['entities']['organizations'])}): {sample['entities']['organizations']}\")\n",
    "        print(f\"  Address ({len(sample['entities']['address'])}): {sample['entities']['address']}\")\n",
    "\n",
    "if train_samples:\n",
    "    print(\"\\n\" + \"#\" * 70)\n",
    "    print(\"TRAINING SAMPLES\")\n",
    "    print(\"#\" * 70)\n",
    "    show_samples(valid_train, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìÅ Output Files:\")\n",
    "print(f\"\\nIndividual results:\")\n",
    "for split in [\"train\", \"test\"]:\n",
    "    for category in CATEGORIES_TO_PROCESS:\n",
    "        json_dir = PROCESSED_DATA_DIR / split / category / \"json\"\n",
    "        if json_dir.exists():\n",
    "            count = len(list(json_dir.glob(\"*.json\")))\n",
    "            print(f\"  - {json_dir}: {count} files\")\n",
    "\n",
    "print(f\"\\nCombined datasets:\")\n",
    "for file_name in [\n",
    "    \"langextract_train.json\",\n",
    "    \"langextract_train.jsonl\",\n",
    "    \"langextract_train_finetuning.jsonl\",\n",
    "    \"langextract_test.json\",\n",
    "    \"langextract_test.jsonl\",\n",
    "    \"langextract_test_finetuning.jsonl\",\n",
    "    \"langextract_statistics.json\"\n",
    "]:\n",
    "    file_path = PROCESSED_DATA_DIR / file_name\n",
    "    if file_path.exists():\n",
    "        print(f\"  ‚úÖ {file_path}\")\n",
    "\n",
    "print(\"\\nüìä Statistics:\")\n",
    "if train_stats:\n",
    "    print(f\"  Train: {train_stats['total_samples']} samples, {train_stats['total_entities']} entities\")\n",
    "if test_stats:\n",
    "    print(f\"  Test: {test_stats['total_samples']} samples, {test_stats['total_entities']} entities\")\n",
    "\n",
    "print(\"\\nüéâ Ready for model training and evaluation!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Review the extracted entities for quality\")\n",
    "print(\"  2. Use langextract_train_finetuning.jsonl for LLM finetuning\")\n",
    "print(\"  3. Use langextract_test.json for model evaluation\")\n",
    "print(\"  4. Compare with other NER approaches (RAG, Prompt Engineering)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
