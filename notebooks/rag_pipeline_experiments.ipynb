{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# RAG Pipeline - Experiment Notebook\n",
        "\n",
        "Notebook for running RAG-based NER extraction experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "from loguru import logger\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from src.config import PROCESSED_DATA_DIR, RESULTS_DIR, NERRagConfig\n",
        "from src.data_processor import DataProcessor\n",
        "from src.rag_pipeline import RAGNERExtractor\n",
        "from src.utils import (\n",
        "    calculate_metrics,\n",
        "    display_metrics,\n",
        "    save_experiment_results,\n",
        "    compare_experiments\n",
        ")\n",
        "\n",
        "logger.info(\"Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-3",
      "metadata": {},
      "source": [
        "## Experiment Configuration\n",
        "\n",
        "Configure your RAG experiment here. Change these settings to try different configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-4",
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"rag_qwen3_4b_top3\"\n",
        "\n",
        "config = NERRagConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    embedding_model=\"BAAI/bge-small-en-v1.5\",\n",
        "    top_k_retrieval=3,\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# EXPERIMENT_NAME = \"rag_qwen3_4b_top5\"\n",
        "\n",
        "# config = NERRagConfig(\n",
        "#     model_name=\"Qwen/Qwen3-4B\",\n",
        "#     embedding_model=\"BAAI/bge-small-en-v1.5\",\n",
        "#     top_k_retrieval=5,\n",
        "#     temperature=0.1\n",
        "# )\n",
        "\n",
        "# EXPERIMENT_NAME = \"rag_qwen3_4b_top1\"\n",
        "\n",
        "# config = NERRagConfig(\n",
        "#     model_name=\"Qwen/Qwen3-4B\",\n",
        "#     embedding_model=\"BAAI/bge-small-en-v1.5\",\n",
        "#     top_k_retrieval=1,\n",
        "#     temperature=0.1\n",
        "# )\n",
        "\n",
        "print(\"RAG Configuration:\")\n",
        "print(f\"  Model: {config.model_name}\")\n",
        "print(f\"  Embedding model: {config.embedding_model}\")\n",
        "print(f\"  Top-k retrieval: {config.top_k_retrieval}\")\n",
        "print(f\"  Temperature: {config.temperature}\")\n",
        "print(f\"  Max tokens: {config.max_new_tokens}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-5",
      "metadata": {},
      "source": [
        "## Load Datasets\n",
        "\n",
        "Load training data as corpus and test data for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load train dataset as corpus\n",
        "train_dataset_path = PROCESSED_DATA_DIR / \"train.json\"\n",
        "if not train_dataset_path.exists():\n",
        "    raise FileNotFoundError(f\"Train dataset not found: {train_dataset_path}\")\n",
        "\n",
        "logger.info(f\"Loading train dataset as corpus from {train_dataset_path}\")\n",
        "corpus = DataProcessor.load_dataset(train_dataset_path)\n",
        "logger.success(f\"Loaded {len(corpus)} corpus documents\")\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset_path = PROCESSED_DATA_DIR / \"test.json\"\n",
        "if not test_dataset_path.exists():\n",
        "    raise FileNotFoundError(f\"Test dataset not found: {test_dataset_path}\")\n",
        "\n",
        "logger.info(f\"Loading test dataset from {test_dataset_path}\")\n",
        "test_dataset = DataProcessor.load_dataset(test_dataset_path)\n",
        "logger.success(f\"Loaded {len(test_dataset)} test samples\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATASET EXAMPLE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Text:\\n{test_dataset[0]['text'][:300]}...\\n\")\n",
        "print(f\"Entities:\\n{json.dumps(test_dataset[0]['entities'], indent=2, ensure_ascii=False)}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {},
      "source": [
        "## Initialize RAG Extractor\n",
        "\n",
        "Load the model, embedding model, and build the FAISS index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-8",
      "metadata": {},
      "outputs": [],
      "source": [
        "extractor = RAGNERExtractor(config=config, corpus=corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {},
      "source": [
        "## Test Single Sample\n",
        "\n",
        "Test on one sample to verify everything works before running the full evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-10",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_text = test_dataset[0][\"text\"]\n",
        "test_label = test_dataset[0][\"entities\"]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SINGLE SAMPLE TEST\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nInput text:\\n{test_text[:300]}...\\n\")\n",
        "print(f\"Ground truth:\\n{json.dumps(test_label, indent=2, ensure_ascii=False)}\\n\")\n",
        "\n",
        "# Test retrieval\n",
        "logger.info(\"Testing retrieval...\")\n",
        "retrieved = extractor.retrieve(test_text)\n",
        "print(f\"\\nRetrieved {len(retrieved)} documents:\")\n",
        "for i, doc in enumerate(retrieved, 1):\n",
        "    print(f\"\\nDocument {i} (score: {doc['retrieval_score']:.4f}):\")\n",
        "    print(f\"  Text: {doc['text'][:150]}...\")\n",
        "    print(f\"  Entities: {doc.get('entities', {})}\")\n",
        "\n",
        "# Test extraction\n",
        "logger.info(\"\\nRunning extraction on test sample...\")\n",
        "prediction = extractor.extract_entities(test_text)\n",
        "\n",
        "print(f\"\\nPrediction:\\n{json.dumps(prediction, indent=2, ensure_ascii=False)}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {},
      "source": [
        "## Run Full Extraction\n",
        "\n",
        "Extract entities from all test samples using RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "texts = [sample[\"text\"] for sample in test_dataset]\n",
        "labels = [sample[\"entities\"] for sample in test_dataset]\n",
        "\n",
        "logger.info(f\"Starting RAG extraction on {len(test_dataset)} samples...\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Running RAG extraction on {len(test_dataset)} samples\")\n",
        "print(f\"Corpus size: {len(corpus)} documents\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "start_time = time.time()\n",
        "predictions = []\n",
        "\n",
        "for i, (text, label) in enumerate(tqdm(zip(texts, labels), desc=\"Extracting entities\", total=len(texts))):\n",
        "    prediction = extractor.extract_entities(text)\n",
        "    predictions.append(prediction)\n",
        "    \n",
        "    # Show first few predictions for debugging\n",
        "    if i < 3:\n",
        "        logger.debug(f\"Sample {i+1}:\")\n",
        "        logger.debug(f\"  Ground truth: {label}\")\n",
        "        logger.debug(f\"  Prediction  : {prediction}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "throughput = len(test_dataset) / elapsed_time if elapsed_time > 0 else 0\n",
        "avg_time = elapsed_time / len(test_dataset)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXTRACTION COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total samples: {len(test_dataset)}\")\n",
        "print(f\"Total time: {elapsed_time:.2f}s\")\n",
        "print(f\"Throughput: {throughput:.2f} samples/s\")\n",
        "print(f\"Avg time per sample: {avg_time:.2f}s\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {},
      "source": [
        "## Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {},
      "outputs": [],
      "source": [
        "logger.info(\"Calculating metrics...\")\n",
        "metrics = calculate_metrics(predictions, labels)\n",
        "\n",
        "display_metrics(metrics, title=f\"METRICS - {EXPERIMENT_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {},
      "source": [
        "## Analyze Sample Results\n",
        "\n",
        "Look at some examples to understand model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze predictions\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SAMPLE PREDICTIONS ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Show first 3 samples\n",
        "for i in range(min(3, len(test_dataset))):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Text: {texts[i][:200]}...\")\n",
        "    print(\"\\nGround Truth:\")\n",
        "    print(json.dumps(labels[i], indent=2, ensure_ascii=False))\n",
        "    print(\"\\nPrediction:\")\n",
        "    print(json.dumps(predictions[i], indent=2, ensure_ascii=False))\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Find errors\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ERROR ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "error_count = 0\n",
        "for i, (pred, truth) in enumerate(zip(predictions, labels)):\n",
        "    has_error = False\n",
        "    for entity_type in [\"person\", \"organizations\", \"address\"]:\n",
        "        pred_set = set(pred.get(entity_type, []))\n",
        "        truth_set = set(truth.get(entity_type, []))\n",
        "        if pred_set != truth_set:\n",
        "            has_error = True\n",
        "            break\n",
        "    \n",
        "    if has_error:\n",
        "        error_count += 1\n",
        "\n",
        "accuracy = (len(test_dataset) - error_count) / len(test_dataset) * 100\n",
        "print(f\"Samples with errors: {error_count} / {len(test_dataset)}\")\n",
        "print(f\"Perfect match accuracy: {accuracy:.2f}%\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {},
      "source": [
        "## Save Experiment Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare config summary\n",
        "config_summary = {\n",
        "    \"model_name\": config.model_name,\n",
        "    \"embedding_model\": config.embedding_model,\n",
        "    \"top_k_retrieval\": config.top_k_retrieval,\n",
        "    \"temperature\": config.temperature,\n",
        "    \"max_new_tokens\": config.max_new_tokens,\n",
        "    \"corpus_size\": len(corpus),\n",
        "}\n",
        "\n",
        "# Prepare performance summary\n",
        "performance = {\n",
        "    \"total_samples\": len(test_dataset),\n",
        "    \"elapsed_time\": round(elapsed_time, 2),\n",
        "    \"throughput\": round(throughput, 2),\n",
        "    \"avg_time_per_sample\": round(avg_time, 2),\n",
        "}\n",
        "\n",
        "# Save results\n",
        "exp_dir = save_experiment_results(\n",
        "    experiment_name=EXPERIMENT_NAME,\n",
        "    config=config_summary,\n",
        "    metrics=metrics,\n",
        "    performance=performance,\n",
        "    predictions=predictions,\n",
        "    texts=texts,\n",
        "    ground_truth=labels\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Experiment results saved to: {exp_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-19",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Free GPU memory and clean up resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-20",
      "metadata": {},
      "outputs": [],
      "source": [
        "extractor.cleanup()\n",
        "logger.success(\"Cleanup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-21",
      "metadata": {},
      "source": [
        "## Compare Multiple Experiments\n",
        "\n",
        "Load and compare results from multiple RAG experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-22",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all experiment directories\n",
        "experiments_dir = RESULTS_DIR / \"prompt_pipeline_experiments\"\n",
        "\n",
        "if experiments_dir.exists():\n",
        "    exp_dirs = [d for d in experiments_dir.iterdir() if d.is_dir()]\n",
        "    \n",
        "    if exp_dirs:\n",
        "        print(f\"Found {len(exp_dirs)} experiments\")\n",
        "        comparison_df = compare_experiments(exp_dirs)\n",
        "        \n",
        "        if comparison_df is not None:\n",
        "            best = comparison_df.iloc[0]\n",
        "            print(f\"\\nBest experiment: {best['experiment']}\")\n",
        "            print(f\"   F1 Score: {best['f1_overall']:.4f}\")\n",
        "            print(f\"   Throughput: {best['throughput']:.2f} samples/s\")\n",
        "    else:\n",
        "        print(\"No experiments found yet\")\n",
        "else:\n",
        "    print(\"Experiments directory doesn't exist yet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-23",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Experiment Templates\n",
        "\n",
        "Copy these configurations into the \"Experiment Configuration\" cell to try different setups:\n",
        "\n",
        "### 1. Top-3 Retrieval (Default)\n",
        "```python\n",
        "EXPERIMENT_NAME = \"rag_top3_default\"\n",
        "config = NERRagConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    top_k_retrieval=3,\n",
        "    temperature=0.1\n",
        ")\n",
        "```\n",
        "\n",
        "### 2. Top-5 Retrieval\n",
        "```python\n",
        "EXPERIMENT_NAME = \"rag_top5\"\n",
        "config = NERRagConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    top_k_retrieval=5,\n",
        "    temperature=0.1\n",
        ")\n",
        "```\n",
        "\n",
        "### 3. Top-1 Retrieval (Single Example)\n",
        "```python\n",
        "EXPERIMENT_NAME = \"rag_top1\"\n",
        "config = NERRagConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    top_k_retrieval=1,\n",
        "    temperature=0.1\n",
        ")\n",
        "```\n",
        "\n",
        "### 4. Different Embedding Model\n",
        "```python\n",
        "EXPERIMENT_NAME = \"rag_bge_large\"\n",
        "config = NERRagConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    embedding_model=\"BAAI/bge-large-en-v1.5\",\n",
        "    top_k_retrieval=3,\n",
        "    temperature=0.1\n",
        ")\n",
        "```\n",
        "\n",
        "### 5. Higher Temperature\n",
        "```python\n",
        "EXPERIMENT_NAME = \"rag_temp_0.7\"\n",
        "config = NERRagConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    top_k_retrieval=3,\n",
        "    temperature=0.7\n",
        ")\n",
        "```\n",
        "\n",
        "### 6. Greedy Decoding\n",
        "```python\n",
        "EXPERIMENT_NAME = \"rag_greedy\"\n",
        "config = NERRagConfig(\n",
        "    model_name=\"Qwen/Qwen3-4B\",\n",
        "    top_k_retrieval=3,\n",
        "    temperature=0.0,\n",
        "    do_sample=False\n",
        ")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
