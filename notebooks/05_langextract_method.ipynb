{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 4: LangExtract for News Article NER Extraction\n",
    "\n",
    "This notebook demonstrates NER extraction using Google's LangExtract library, specifically designed for extracting structured information from text with high accuracy.\n",
    "\n",
    "## Overview\n",
    "- **Approach**: Schema-guided extraction with multiple passes\n",
    "- **Model**: Gemini 2.0 Flash (or other Gemini models)\n",
    "- **Key Features**:\n",
    "  - Multiple extraction passes for better recall\n",
    "  - Parallel processing for speed\n",
    "  - Smart chunking for long documents\n",
    "  - Interactive visualizations\n",
    "  - JSONL output for portability\n",
    "- **Advantages**: \n",
    "  - High accuracy with world knowledge\n",
    "  - Handles long documents efficiently\n",
    "  - Rich entity attributes\n",
    "  - No training required\n",
    "- **Disadvantages**:\n",
    "  - Requires Google API key\n",
    "  - API costs per request\n",
    "  - Internet connection required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangExtract if not already installed\n",
    "!pip install -q langextract google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.config import NERConfig, PROCESSED_DATA_DIR, RESULTS_DIR\n",
    "from src.data_loader import NERDataLoader\n",
    "from src.langextract_pipeline import LangExtractNERExtractor\n",
    "from src.evaluation import NEREvaluator\n",
    "from src.benchmark import NERBenchmark\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check for API key\n",
    "if not os.getenv('GOOGLE_API_KEY'):\n",
    "    print(\"‚ö†Ô∏è Warning: GOOGLE_API_KEY not found in environment\")\n",
    "    print(\"Please set it in .env file or set it now:\")\n",
    "    print(\"   export GOOGLE_API_KEY='your-api-key-here'\")\n",
    "    print(\"\\nGet your API key from: https://ai.google.dev/\")\n",
    "else:\n",
    "    print(\"‚úì Google API key found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = NERConfig()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Entity types: {config.entity_types}\")\n",
    "print(f\"  Model: Gemini 2.0 Flash (via LangExtract)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed dataset\n",
    "val_dataset = NERDataLoader.load_json_dataset(PROCESSED_DATA_DIR / \"validation.json\")\n",
    "test_dataset = NERDataLoader.load_json_dataset(PROCESSED_DATA_DIR / \"test.json\")\n",
    "\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample sample:\")\n",
    "print(f\"Text: {val_dataset[0]['text'][:200]}...\")\n",
    "print(f\"Entities: {val_dataset[0]['entities']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize LangExtract Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize extractor\n",
    "extractor = LangExtractNERExtractor(\n",
    "    config=config,\n",
    "    model_id=\"gemini-2.0-flash-exp\"  # Use Gemini 2.0 Flash (fast and cost-effective)\n",
    ")\n",
    "\n",
    "print(\"‚úì LangExtract extractor initialized!\")\n",
    "print(f\"\\nPrompt:\")\n",
    "print(extractor.prompt)\n",
    "print(f\"\\nNumber of examples: {len(extractor.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test on Sample Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a few examples\n",
    "num_examples = 3\n",
    "\n",
    "for i, sample in enumerate(val_dataset[:num_examples]):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    text = sample['text']\n",
    "    ground_truth = sample['entities']\n",
    "    \n",
    "    print(f\"\\nText: {text[:300]}...\\n\")\n",
    "    \n",
    "    # Extract entities with LangExtract\n",
    "    print(\"Extracting entities...\")\n",
    "    predicted = extractor.extract_entities(\n",
    "        text,\n",
    "        extraction_passes=2,  # Multiple passes for better recall\n",
    "        max_workers=5,\n",
    "        max_char_buffer=2000\n",
    "    )\n",
    "    \n",
    "    print(\"Ground Truth:\")\n",
    "    print(json.dumps(ground_truth, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    print(\"\\nPredicted:\")\n",
    "    print(json.dumps(predicted, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract with Detailed Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test detailed extraction with attributes\n",
    "sample_text = val_dataset[0]['text']\n",
    "\n",
    "print(\"Extracting with detailed attributes...\\n\")\n",
    "detailed_result = extractor.extract_with_details(\n",
    "    sample_text,\n",
    "    extraction_passes=2\n",
    ")\n",
    "\n",
    "print(\"Detailed Extraction Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for entity_type in [\"person\", \"organizations\", \"address\"]:\n",
    "    entities = detailed_result[entity_type]\n",
    "    print(f\"\\n{entity_type.upper()} ({len(entities)} entities):\")\n",
    "    for entity in entities[:5]:  # Show first 5\n",
    "        attrs = \", \".join(f\"{k}={v}\" for k, v in entity['attributes'].items())\n",
    "        attrs_str = f\" [{attrs}]\" if attrs else \"\"\n",
    "        print(f\"  - {entity['text']}{attrs_str}\")\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "print(json.dumps(detailed_result['statistics'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Validation Set\n",
    "\n",
    "**Note**: This will make API calls for each sample. Start with a small subset to estimate costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a small subset for testing\n",
    "# Adjust this number based on your API quota and budget\n",
    "EVAL_SUBSET_SIZE = 50  # Start small, increase if needed\n",
    "\n",
    "val_subset = val_dataset[:EVAL_SUBSET_SIZE]\n",
    "\n",
    "print(f\"Evaluating on {len(val_subset)} samples...\")\n",
    "print(f\"Estimated API calls: ~{len(val_subset) * 2} (with 2 extraction passes)\")\n",
    "print(\"\\nThis may take a few minutes...\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "predictions, ground_truth = extractor.evaluate_on_dataset(val_subset)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = NEREvaluator(entity_types=config.entity_types)\n",
    "results = evaluator.evaluate_all(predictions, ground_truth)\n",
    "\n",
    "# Print results\n",
    "evaluator.print_results(results)\n",
    "\n",
    "# Save results\n",
    "results_path = RESULTS_DIR / \"langextract_validation.json\"\n",
    "evaluator.save_results(results, results_path)\n",
    "print(f\"Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Benchmark on Test Set (Optional)\n",
    "\n",
    "**Warning**: This will process the full test set and may incur significant API costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run full benchmark\n",
    "# TEST_SUBSET_SIZE = 100\n",
    "# test_subset = test_dataset[:TEST_SUBSET_SIZE]\n",
    "\n",
    "# benchmark = NERBenchmark(config=config)\n",
    "# test_results = benchmark.run_benchmark(\n",
    "#     method_name=\"LangExtract\",\n",
    "#     extractor=extractor,\n",
    "#     test_dataset=test_subset,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# # Save benchmark results\n",
    "# benchmark.save_results(RESULTS_DIR / \"langextract\")\n",
    "\n",
    "print(\"‚ö†Ô∏è Full benchmark commented out to avoid unexpected API costs.\")\n",
    "print(\"Uncomment the code above to run the full benchmark.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Interactive Visualization\n",
    "\n",
    "LangExtract can create beautiful interactive HTML visualizations of extracted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a few articles and create visualization\n",
    "vis_samples = val_dataset[:10]\n",
    "vis_texts = [s['text'] for s in vis_samples]\n",
    "\n",
    "# Save annotated documents\n",
    "jsonl_path = RESULTS_DIR / \"langextract_samples.jsonl\"\n",
    "extractor.save_annotated_documents(\n",
    "    vis_texts,\n",
    "    output_path=str(jsonl_path),\n",
    "    extraction_passes=2\n",
    ")\n",
    "\n",
    "# Create visualization\n",
    "html_path = RESULTS_DIR / \"langextract_visualization.html\"\n",
    "extractor.create_visualization(\n",
    "    jsonl_path=str(jsonl_path),\n",
    "    output_html_path=str(html_path)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Visualization created!\")\n",
    "print(f\"Open {html_path} in your browser to explore the results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze Extraction Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the extraction results\n",
    "stats = LangExtractNERExtractor.analyze_extraction_statistics(str(jsonl_path))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTION STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDocuments processed: {stats['total_documents']}\")\n",
    "print(f\"Total characters: {stats['total_characters']:,}\")\n",
    "print(f\"Total extractions: {stats['total_extractions']}\")\n",
    "print(f\"Extractions per document: {stats['extractions_per_document']:.1f}\")\n",
    "\n",
    "print(\"\\nExtractions by class:\")\n",
    "for entity_class, count in stats['class_counts'].items():\n",
    "    percentage = (count / stats['total_extractions']) * 100\n",
    "    print(f\"  {entity_class}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nUnique entities:\")\n",
    "for entity_class, count in stats['unique_entities'].items():\n",
    "    print(f\"  {entity_class}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare with Other Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load results from other methods (if available)\n",
    "comparison_data = []\n",
    "\n",
    "# LangExtract results\n",
    "comparison_data.append({\n",
    "    \"Method\": \"LangExtract\",\n",
    "    \"Exact Match\": results['exact_match_accuracy'],\n",
    "    \"Macro F1\": results['partial_match_metrics']['macro_avg']['f1'],\n",
    "    \"Samples\": len(val_subset),\n",
    "    \"Notes\": \"Gemini API, 2 passes\"\n",
    "})\n",
    "\n",
    "# Try to load other methods\n",
    "other_methods = {\n",
    "    \"Prompt Engineering\": RESULTS_DIR / \"prompt_engineering\" / \"Prompt Engineering_results.json\",\n",
    "    \"RAG\": RESULTS_DIR / \"rag\" / \"RAG_results.json\",\n",
    "    \"Fine-tuning\": RESULTS_DIR / \"finetuning\" / \"Fine-tuning_results.json\",\n",
    "}\n",
    "\n",
    "for method_name, result_path in other_methods.items():\n",
    "    if result_path.exists():\n",
    "        with open(result_path, 'r') as f:\n",
    "            method_results = json.load(f)\n",
    "        comparison_data.append({\n",
    "            \"Method\": method_name,\n",
    "            \"Exact Match\": method_results['exact_match_accuracy'],\n",
    "            \"Macro F1\": method_results['partial_match_metrics']['macro_avg']['f1'],\n",
    "            \"Samples\": \"Full test set\",\n",
    "            \"Notes\": \"-\"\n",
    "        })\n",
    "\n",
    "# Display comparison\n",
    "if len(comparison_data) > 1:\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARISON WITH OTHER METHODS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    print(df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No other method results found for comparison.\")\n",
    "    print(\"Run other method notebooks first to enable comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LANGEXTRACT METHOD: KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Performance on {len(val_subset)} samples:\")\n",
    "print(f\"  - Exact Match Accuracy: {results['exact_match_accuracy']:.2%}\")\n",
    "print(f\"  - Macro F1 Score: {results['partial_match_metrics']['macro_avg']['f1']:.2%}\")\n",
    "\n",
    "print(\"\\n‚úÖ Strengths:\")\n",
    "print(\"  - High accuracy with world knowledge enrichment\")\n",
    "print(\"  - Multiple extraction passes improve recall\")\n",
    "print(\"  - Rich entity attributes (role, context, type)\")\n",
    "print(\"  - Handles long documents efficiently with smart chunking\")\n",
    "print(\"  - Beautiful interactive visualizations\")\n",
    "print(\"  - JSONL format for portability\")\n",
    "print(\"  - No training required\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Considerations:\")\n",
    "print(\"  - Requires Google API key and internet connection\")\n",
    "print(\"  - API costs per request (though Flash model is cost-effective)\")\n",
    "print(\"  - Slower than local models due to API calls\")\n",
    "print(\"  - Subject to API rate limits\")\n",
    "\n",
    "print(\"\\nüí° Best Use Cases:\")\n",
    "print(\"  - One-time or periodic extraction tasks\")\n",
    "print(\"  - When high accuracy is critical\")\n",
    "print(\"  - Long documents or complex news articles\")\n",
    "print(\"  - When you need rich entity attributes\")\n",
    "print(\"  - Exploratory analysis with visualizations\")\n",
    "\n",
    "print(\"\\nüí∞ Cost Optimization Tips:\")\n",
    "print(\"  - Use gemini-2.0-flash-exp for cost-effectiveness\")\n",
    "print(\"  - Reduce extraction_passes for simpler texts\")\n",
    "print(\"  - Batch process documents to minimize overhead\")\n",
    "print(\"  - Use max_char_buffer wisely for your text length\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export for Comparison\n",
    "\n",
    "Save results in the same format as other methods for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in benchmark format\n",
    "langextract_dir = RESULTS_DIR / \"langextract\"\n",
    "langextract_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add method name to results\n",
    "results['method_name'] = 'LangExtract'\n",
    "results['model_info'] = 'Gemini 2.0 Flash Experimental'\n",
    "results['extraction_passes'] = 2\n",
    "\n",
    "# Save results\n",
    "with open(langextract_dir / \"LangExtract_results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Save predictions\n",
    "with open(langextract_dir / \"predictions.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth,\n",
    "        'sample_count': len(predictions)\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úì Results saved for comparison!\")\n",
    "print(f\"Location: {langextract_dir}\")\n",
    "print(\"\\nYou can now run the comparison notebook to compare with other methods.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
