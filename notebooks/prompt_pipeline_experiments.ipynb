{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Pipeline - Experiment Notebook\n",
    "\n",
    "Notebook for running prompt engineering NER extraction experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.config import (\n",
    "    PROCESSED_DATA_DIR,\n",
    "    RESULTS_DIR,\n",
    "    ExtractionMode,\n",
    "    NERPromptEngineeringConfig,\n",
    ")\n",
    "from src.data_processor import DataProcessor\n",
    "from src.prompt_engineering import PromptNERExtractor\n",
    "from src.utils import (\n",
    "    calculate_metrics,\n",
    "    display_metrics,\n",
    "    save_experiment_results,\n",
    "    compare_experiments\n",
    ")\n",
    "\n",
    "logger.info(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "Configure your experiment here. Change these settings to try different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"qwen3_4b_raw_no_thinking\"\n",
    "\n",
    "config = NERPromptEngineeringConfig(\n",
    "    model_name=\"Qwen/Qwen3-4B\",\n",
    "    extraction_mode=ExtractionMode.RAW,\n",
    "    enable_thinking=False\n",
    ")\n",
    "\n",
    "# EXPERIMENT_NAME = \"qwen3_4b_raw_thinking\"\n",
    "\n",
    "# config = NERPromptEngineeringConfig(\n",
    "#     model_name=\"Qwen/Qwen3-4B\",\n",
    "#     extraction_mode=ExtractionMode.RAW,\n",
    "#     enable_thinking=True\n",
    "# )\n",
    "\n",
    "# EXPERIMENT_NAME = \"qwen3_4b_structured_no_thinking\"\n",
    "\n",
    "# config = NERPromptEngineeringConfig(\n",
    "#     model_name=\"Qwen/Qwen3-4B\",\n",
    "#     extraction_mode=ExtractionMode.STRUCTURED_OUTPUT,\n",
    "#     enable_thinking=False\n",
    "# )\n",
    "\n",
    "# EXPERIMENT_NAME = \"qwen3_4b_structured_thinking\"\n",
    "\n",
    "# config = NERPromptEngineeringConfig(\n",
    "#     model_name=\"Qwen/Qwen3-4B\",\n",
    "#     extraction_mode=ExtractionMode.STRUCTURED_OUTPUT,\n",
    "#     enable_thinking=True\n",
    "# )\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Extraction mode: {config.extraction_mode.value}\")\n",
    "print(f\"  Add schema: {config.add_schema}\")\n",
    "print(f\"  Enable thinking: {config.enable_thinking}\")\n",
    "print(f\"  Temperature: {config.temperature}\")\n",
    "print(f\"  Max tokens: {config.max_new_tokens}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = PROCESSED_DATA_DIR / \"test.json\"\n",
    "\n",
    "if not test_dataset_path.exists():\n",
    "    raise FileNotFoundError(f\"Test dataset not found: {test_dataset_path}\")\n",
    "\n",
    "logger.info(f\"Loading test dataset from {test_dataset_path}\")\n",
    "test_dataset = DataProcessor.load_dataset(test_dataset_path)\n",
    "\n",
    "logger.success(f\"Loaded {len(test_dataset)} test samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Text:\\n{test_dataset[0]['text'][:300]}...\\n\")\n",
    "print(f\"Entities:\\n{json.dumps(test_dataset[0]['entities'], indent=2, ensure_ascii=False)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Extractor\n",
    "\n",
    "Load the model and prepare the extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = PromptNERExtractor(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Single Sample\n",
    "\n",
    "Test on one sample to verify everything works before running the full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_dataset[0][\"text\"]\n",
    "test_label = test_dataset[0][\"entities\"]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SINGLE SAMPLE TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nInput text:\\n{test_text[:300]}...\\n\")\n",
    "print(f\"Ground truth:\\n{json.dumps(test_label, indent=2, ensure_ascii=False)}\\n\")\n",
    "\n",
    "logger.info(\"Running extraction on test sample...\")\n",
    "prediction = extractor.extract_entities(test_text)\n",
    "\n",
    "print(f\"Prediction:\\n{json.dumps(prediction, indent=2, ensure_ascii=False)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Extraction\n",
    "\n",
    "Extract entities from all test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "texts = [sample[\"text\"] for sample in test_dataset]\n",
    "labels = [sample[\"entities\"] for sample in test_dataset]\n",
    "\n",
    "logger.info(f\"Starting extraction on {len(test_dataset)} samples...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Running extraction on {len(test_dataset)} samples\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "predictions = []\n",
    "\n",
    "for i, (text, label) in enumerate(tqdm(zip(texts, labels), desc=\"Extracting entities\", total=len(texts))):\n",
    "    prediction = extractor.extract_entities(text)\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    # Show first few predictions for debugging\n",
    "    if i < 3:\n",
    "        logger.debug(f\"Sample {i+1}:\")\n",
    "        logger.debug(f\"  Ground truth: {label}\")\n",
    "        logger.debug(f\"  Prediction  : {prediction}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "throughput = len(test_dataset) / elapsed_time if elapsed_time > 0 else 0\n",
    "avg_time = elapsed_time / len(test_dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXTRACTION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total samples: {len(test_dataset)}\")\n",
    "print(f\"Total time: {elapsed_time:.2f}s\")\n",
    "print(f\"Throughput: {throughput:.2f} samples/s\")\n",
    "print(f\"Avg time per sample: {avg_time:.2f}s\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Calculating metrics...\")\n",
    "metrics = calculate_metrics(predictions, labels)\n",
    "\n",
    "display_metrics(metrics, title=f\"METRICS - {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Sample Results\n",
    "\n",
    "Look at some examples to understand model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE PREDICTIONS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show first 3 samples\n",
    "for i in range(min(3, len(test_dataset))):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Text: {texts[i][:200]}...\")\n",
    "    print(\"\\nGround Truth:\")\n",
    "    print(json.dumps(labels[i], indent=2, ensure_ascii=False))\n",
    "    print(\"\\nPrediction:\")\n",
    "    print(json.dumps(predictions[i], indent=2, ensure_ascii=False))\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Find errors\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "error_count = 0\n",
    "for i, (pred, truth) in enumerate(zip(predictions, labels)):\n",
    "    has_error = False\n",
    "    for entity_type in [\"person\", \"organizations\", \"address\"]:\n",
    "        pred_set = set(pred.get(entity_type, []))\n",
    "        truth_set = set(truth.get(entity_type, []))\n",
    "        if pred_set != truth_set:\n",
    "            has_error = True\n",
    "            break\n",
    "    \n",
    "    if has_error:\n",
    "        error_count += 1\n",
    "\n",
    "accuracy = (len(test_dataset) - error_count) / len(test_dataset) * 100\n",
    "print(f\"Samples with errors: {error_count} / {len(test_dataset)}\")\n",
    "print(f\"Perfect match accuracy: {accuracy:.2f}%\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare config summary\n",
    "config_summary = {\n",
    "    \"model_name\": config.model_name,\n",
    "    \"extraction_mode\": config.extraction_mode.value,\n",
    "    \"add_schema\": config.add_schema,\n",
    "    \"enable_thinking\": config.enable_thinking,\n",
    "    \"temperature\": config.temperature,\n",
    "    \"max_new_tokens\": config.max_new_tokens,\n",
    "}\n",
    "\n",
    "# Prepare performance summary\n",
    "performance = {\n",
    "    \"total_samples\": len(test_dataset),\n",
    "    \"elapsed_time\": round(elapsed_time, 2),\n",
    "    \"throughput\": round(throughput, 2),\n",
    "    \"avg_time_per_sample\": round(avg_time, 2),\n",
    "}\n",
    "\n",
    "# Save results\n",
    "exp_dir = save_experiment_results(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    config=config_summary,\n",
    "    metrics=metrics,\n",
    "    performance=performance,\n",
    "    predictions=predictions,\n",
    "    texts=texts,\n",
    "    ground_truth=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Experiment results saved to: {exp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Free GPU memory and clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.cleanup()\n",
    "logger.success(\"Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Multiple Experiments\n",
    "\n",
    "Load and compare results from multiple experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all experiment directories\n",
    "experiments_dir = RESULTS_DIR / \"prompt_pipeline_experiments\"\n",
    "\n",
    "if experiments_dir.exists():\n",
    "    exp_dirs = [d for d in experiments_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if exp_dirs:\n",
    "        print(f\"Found {len(exp_dirs)} experiments\")\n",
    "        comparison_df = compare_experiments(exp_dirs)\n",
    "        \n",
    "        if comparison_df is not None:\n",
    "            best = comparison_df.iloc[0]\n",
    "            print(f\"\\nBest experiment: {best['experiment']}\")\n",
    "            print(f\"   F1 Score: {best['f1_overall']:.4f}\")\n",
    "            print(f\"   Throughput: {best['throughput']:.2f} samples/s\")\n",
    "    else:\n",
    "        print(\"No experiments found yet\")\n",
    "else:\n",
    "    print(\"Experiments directory doesn't exist yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Experiment Templates\n",
    "\n",
    "Copy these configurations into the \"Experiment Configuration\" cell to try different setups:\n",
    "\n",
    "### 1. Baseline - RAW Chat Mode\n",
    "```python\n",
    "EXPERIMENT_NAME = \"raw_chat_baseline\"\n",
    "config = NERPromptEngineeringConfig(\n",
    "    extraction_mode=ExtractionMode.RAW,\n",
    "    add_schema=False,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. RAW with Schema\n",
    "```python\n",
    "EXPERIMENT_NAME = \"raw_chat_with_schema\"\n",
    "config = NERPromptEngineeringConfig(\n",
    "    extraction_mode=ExtractionMode.RAW,\n",
    "    add_schema=True,\n",
    "    enable_thinking=False,\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Structured Output\n",
    "```python\n",
    "EXPERIMENT_NAME = \"structured_output\"\n",
    "config = NERPromptEngineeringConfig(\n",
    "    extraction_mode=ExtractionMode.STRUCTURED_OUTPUT,\n",
    "    add_schema=False,\n",
    ")\n",
    "```\n",
    "\n",
    "### 4. With Thinking Mode\n",
    "```python\n",
    "EXPERIMENT_NAME = \"raw_chat_thinking\"\n",
    "config = NERPromptEngineeringConfig(\n",
    "    extraction_mode=ExtractionMode.RAW,\n",
    "    add_schema=False,\n",
    "    enable_thinking=True,\n",
    ")\n",
    "```\n",
    "\n",
    "### 5. Higher Temperature\n",
    "```python\n",
    "EXPERIMENT_NAME = \"raw_chat_temp_0.7\"\n",
    "config = NERPromptEngineeringConfig(\n",
    "    extraction_mode=ExtractionMode.RAW,\n",
    "    temperature=0.7,\n",
    ")\n",
    "```\n",
    "\n",
    "### 6. Lower Temperature (Greedy)\n",
    "```python\n",
    "EXPERIMENT_NAME = \"raw_chat_temp_0.0\"\n",
    "config = NERPromptEngineeringConfig(\n",
    "    extraction_mode=ExtractionMode.RAW,\n",
    "    temperature=0.0,\n",
    "    do_sample=False,\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
