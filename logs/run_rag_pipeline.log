2025-11-24 03:02:09.832 | INFO     | __main__:<module>:17 - ================================================================================
2025-11-24 03:02:09.832 | INFO     | __main__:<module>:18 - RAG-based NER Evaluation
2025-11-24 03:02:09.832 | INFO     | __main__:<module>:19 - ================================================================================
2025-11-24 03:02:09.835 | INFO     | __main__:<module>:32 - Loaded 100 corpus documents
2025-11-24 03:02:09.835 | INFO     | __main__:<module>:35 - Loading test dataset from /data2/samdv/test/data/processed/test.json
2025-11-24 03:02:09.837 | INFO     | __main__:<module>:37 - Loaded 100 test samples
2025-11-24 03:02:09.837 | INFO     | __main__:<module>:41 - Model: Qwen/Qwen3-4B-Instruct-2507
2025-11-24 03:02:09.837 | INFO     | __main__:<module>:42 - Embedding model: AITeamVN/Vietnamese_Embedding_v2
2025-11-24 03:02:09.837 | INFO     | __main__:<module>:43 - Top-k retrieval: 3
2025-11-24 03:02:09.837 | INFO     | __main__:<module>:44 - 
2025-11-24 03:02:09.837 | INFO     | src.rag_pipeline:__init__:37 - Loading embedding model: AITeamVN/Vietnamese_Embedding_v2
[2025-11-24 03:02:09] INFO SentenceTransformer.py:219: Use pytorch device_name: cuda:0
[2025-11-24 03:02:09] INFO SentenceTransformer.py:227: Load pretrained SentenceTransformer: AITeamVN/Vietnamese_Embedding_v2
2025-11-24 03:02:15.612 | INFO     | src.rag_pipeline:__init__:44 - Loading reranker model: AITeamVN/Vietnamese_Reranker
2025-11-24 03:02:18.326 | SUCCESS  | src.rag_pipeline:__init__:50 - Reranker loaded as transformer model
2025-11-24 03:02:18.326 | INFO     | src.rag_pipeline:_load_llm:72 - Loading LLM with vLLM: Qwen/Qwen3-4B-Instruct-2507
INFO 11-24 03:02:19 [utils.py:253] non-default args: {'max_model_len': 2048, 'gpu_memory_utilization': 0.3, 'kv_cache_memory_bytes': 1073741824, 'disable_log_stats': True, 'model': 'Qwen/Qwen3-4B-Instruct-2507'}
INFO 11-24 03:02:20 [model.py:631] Resolved architecture: Qwen3ForCausalLM
INFO 11-24 03:02:20 [model.py:1745] Using max model len 2048
INFO 11-24 03:02:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 11-24 03:02:23 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
(EngineCore_DP0 pid=1868832) INFO 11-24 03:02:28 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-4B-Instruct-2507', speculative_config=None, tokenizer='Qwen/Qwen3-4B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-4B-Instruct-2507, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=1868832) INFO 11-24 03:02:29 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.1.3:51717 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=1868832) INFO 11-24 03:02:29 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1868832) INFO 11-24 03:02:30 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-4B-Instruct-2507...
(EngineCore_DP0 pid=1868832) INFO 11-24 03:02:30 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=1868832) INFO 11-24 03:02:30 [cuda.py:427] Using FLASH_ATTN backend.
Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:14<00:28, 14.04s/it]
Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:21<00:10, 10.20s/it]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:22<00:00,  5.82s/it]
Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:22<00:00,  7.39s/it]
(EngineCore_DP0 pid=1868832) 
(EngineCore_DP0 pid=1868832) INFO 11-24 03:02:54 [default_loader.py:314] Loading weights took 22.17 seconds
(EngineCore_DP0 pid=1868832) INFO 11-24 03:02:55 [gpu_model_runner.py:3338] Model loading took 7.6065 GiB memory and 24.213361 seconds
(EngineCore_DP0 pid=1868832) INFO 11-24 03:03:03 [backends.py:631] Using cache directory: /home/samdv/.cache/vllm/torch_compile_cache/a5311017f7/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1868832) INFO 11-24 03:03:03 [backends.py:647] Dynamo bytecode transform time: 8.27 s
(EngineCore_DP0 pid=1868832) INFO 11-24 03:03:09 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.324 s
(EngineCore_DP0 pid=1868832) INFO 11-24 03:03:10 [monitor.py:34] torch.compile takes 13.59 s in total
(EngineCore_DP0 pid=1868832) INFO 11-24 03:03:11 [gpu_worker.py:312] Initial free memory 15.20 GiB, reserved 1.00 GiB memory for KV Cache as specified by kv_cache_memory_bytes config and skipped memory profiling. This does not respect the gpu_memory_utilization config. Only use kv_cache_memory_bytes config when you want manual control of KV cache memory size. If OOM'ed, check the difference of initial free memory between the current run and the previous run where kv_cache_memory_bytes is suggested and update it correspondingly.
(EngineCore_DP0 pid=1868832) INFO 11-24 03:03:11 [kv_cache_utils.py:1229] GPU KV cache size: 7,280 tokens
(EngineCore_DP0 pid=1868832) INFO 11-24 03:03:11 [kv_cache_utils.py:1234] Maximum concurrency for 2,048 tokens per request: 3.55x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|████████████████████████████████████| 51/51 [00:07<00:00,  7.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|███████████████████████████████████████████████████████| 35/35 [00:02<00:00, 16.78it/s]
(EngineCore_DP0 pid=1868832) INFO 11-24 03:03:21 [gpu_model_runner.py:4244] Graph capturing finished in 10 secs, took 0.18 GiB
(EngineCore_DP0 pid=1868832) INFO 11-24 03:03:21 [core.py:250] init engine (profile, create kv cache, warmup model) took 26.70 seconds
INFO 11-24 03:03:23 [llm.py:352] Supported tasks: ['generate']
2025-11-24 03:03:23.672 | SUCCESS  | src.rag_pipeline:_load_llm:87 - LLM loaded successfully with vLLM
2025-11-24 03:03:23.672 | INFO     | src.rag_pipeline:build_index:96 - Building FAISS index...
Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.67s/it]
2025-11-24 03:03:30.391 | SUCCESS  | src.rag_pipeline:build_index:111 - Index built with 100 documents
2025-11-24 03:03:30.391 | INFO     | __main__:<module>:51 - Starting RAG extraction...
Extracting entities:   0%|                                                                                | 0/100 [00:00<?, ?it/s]2025-11-24 03:03:30.392 | INFO     | src.rag_pipeline:extract_entities:258 - Starting RAG-based entity extraction
Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.45it/s]
2025-11-24 03:03:30.468 | DEBUG    | src.rag_pipeline:retrieve:164 - Reranking 9 documents                  | 0/1 [00:00<?, ?it/s]
2025-11-24 03:03:44.122 | DEBUG    | src.rag_pipeline:rerank:203 - Top rerank scores: [9.750243186950684, -7.728096961975098, -7.843038082122803]
Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 238.30it/s]
Processed prompts: 100%|█████████████████████| 1/1 [00:01<00:00,  1.71s/it, est. speed input: 471.28 toks/s, output: 22.78 toks/s]
2025-11-24 03:03:45.840 | DEBUG    | src.rag_pipeline:_query_vllm_model:297 - vLLM response: {"person": [], "organizations": ["Cơ quan quản lý xây dựng", "UBND cấp huyện", "cơ quan tư vấn thiết kế"], "address": []}
2025-11-24 03:03:45.840 | INFO     | src.rag_pipeline:extract_entities:276 - RAG extraction completed in 15.45s
2025-11-24 03:03:45.840 | DEBUG    | __main__:<module>:58 - Ground truth: {'person': ['Thủ tướng'], 'organizations': ['UBND cấp huyện'], 'address': []}
2025-11-24 03:03:45.840 | DEBUG    | __main__:<module>:59 - Prediction  : {'person': [], 'organizations': ['Cơ quan quản lý xây dựng', 'UBND cấp huyện', 'cơ quan tư vấn thiết kế'], 'address': []}
Extracting entities:   1%|▋                                                                       | 1/100 [00:15<25:29, 15.45s/it]2025-11-24 03:03:45.840 | INFO     | src.rag_pipeline:extract_entities:258 - Starting RAG-based entity extraction
Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 44.10it/s]
2025-11-24 03:03:45.867 | DEBUG    | src.rag_pipeline:retrieve:164 - Reranking 9 documents                  | 0/1 [00:00<?, ?it/s]
2025-11-24 03:03:51.156 | DEBUG    | src.rag_pipeline:rerank:203 - Top rerank scores: [5.325591564178467, 0.8335598111152649, 0.4286525845527649]
Adding requests: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 322.81it/s]
Processed prompts: 100%|█████████████████████| 1/1 [00:01<00:00,  1.17s/it, est. speed input: 635.46 toks/s, output: 30.67 toks/s]
2025-11-24 03:03:52.334 | DEBUG    | src.rag_pipeline:_query_vllm_model:297 - vLLM response: {"person": [], "organizations": ["Đoàn Thanh tra liên ngành tỉnh Gia Lai", "NLĐ"], "address": ["Gia Lai"]}
2025-11-24 03:03:52.334 | INFO     | src.rag_pipeline:extract_entities:276 - RAG extraction completed in 6.49s
2025-11-24 03:03:52.334 | DEBUG    | __main__:<module>:58 - Ground truth: {'person': [], 'organizations': ['Đoàn Thanh tra liên ngành tỉnh Gia Lai', 'NLĐ'], 'address': ['Gia Lai']}
2025-11-24 03:03:52.334 | DEBUG    | __main__:<module>:59 - Prediction  : {'person': [], 'organizations': ['Đoàn Thanh tra liên ngành tỉnh Gia Lai', 'NLĐ'], 'address': ['Gia Lai']}
Extracting entities:   2%|█▍                                                                      | 2/100 [00:21<16:37, 10.18s/it]2025-11-24 03:03:52.334 | INFO     | src.rag_pipeline:extract_entities:258 - Starting RAG-based entity extraction
Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.09it/s]
2025-11-24 03:03:52.428 | DEBUG    | src.rag_pipeline:retrieve:164 - Reranking 9 documents                  | 0/1 [00:00<?, ?it/s]
2025-11-24 03:04:13.710 | DEBUG    | src.rag_pipeline:rerank:203 - Top rerank scores: [11.443767547607422, 6.200070381164551, 5.357513904571533]